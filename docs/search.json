[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Warmup_Feb15.html",
    "href": "Warmup_Feb15.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#Meeting the Palmer Penguins Warmup Activity Breanna Guo 2/14/24\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\n\n\ndf.columns\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\nPart A: Make a summary table. How does the mean mass of penguins vary by species and sex?\n\nmeanMass = df.groupby([\"Species\", 'Sex'])[\"Body Mass (g)\"].mean()\nmeanMass\n\nSpecies                                    Sex   \nAdelie Penguin (Pygoscelis adeliae)        FEMALE    3368.835616\n                                           MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica)  FEMALE    3527.205882\n                                           MALE      3938.970588\nGentoo penguin (Pygoscelis papua)          .         4875.000000\n                                           FEMALE    4679.741379\n                                           MALE      5484.836066\nName: Body Mass (g), dtype: float64\n\n\nPart B: Make a scatterplot of clumen length against culmen depth, with the color of each point corresponding to the penguin species\n\n#df.plot.scatter(x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species', colormap = 'jet')\n\nsns.scatterplot(df, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species')"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Penguins/Blog1Penguins.html",
    "href": "posts/Penguins/Blog1Penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n2/22/2024\n\n# Accessing the training data\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n1. Explore\n\nConstruct two interesting displayed figure and at least one interesting displayed summary table. Include discussino of figures and table- explain what is learned about the data from the products\n\n\n\nStarting with Data Preparation:\n\nConverting “Sex” and “Island” columns into binary values.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nNext, in variable selection, to narrow the options down to just three variables, one thing that can be done is remove variables with low variance. Choosing to set a threshold at 80%, for example, removes variables where 80% or more observations share the same variable value.\n\nfrom sklearn.feature_selection import VarianceThreshold\nthresh = VarianceThreshold(threshold = (0.8*(1-0.8)))\nhighVar = thresh.fit_transform(X_train)\nhighVar.shape, X_train.shape\n\n((256, 10), (256, 14))\n\n\n\n\nRemoving features with less than 20% variance got rid of 4 out of 15 variables. As variance increases, I am interested in when and where the msot variables no longer make the threshold\n\nthresh1 = VarianceThreshold(threshold = (0.5*(1-0.5)))\nhighVar1 = thresh1.fit_transform(X_train)\nhighVar1.shape, X_train.shape\n\n((256, 6), (256, 14))\n\n\n\n\nAt around 50% variance, the majority of variables no longer reach the variance threshold. Cool! I will probably use a threshold around 30% variance\n\nthresh30 = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30 = thresh30.fit_transform(X_train)\nhighVar30.shape, X_train.shape\n\n((256, 10), (256, 14))\n\n\n\n\nNext, using Univariate feature selection, we can use an F-test to retreive the three best features for the dataset, estimating the degree of linear dependency between the variables.\n\n#from sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n#X_train, y_train = load_iris(return_X_y = True)\n#X_train.shape\n# Create and fit selector\nX_new = SelectKBest(f_classif, k = 3)\nX_new.fit(X_train, y_train)\n\n\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [9] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\nSelectKBest(k=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SelectKBestSelectKBest(k=3)\n\n\n\n\nNow that we have the 3 best features, we have to create a new dataframe with the columns we want in it\n\n# Get columns to keep and create new dataframe\ncols = X_new.get_support(indices = True)\nX_train_new = X_train.iloc[:,cols]\n\nX_train_new.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n40.9\n16.6\n187.0\n\n\n1\n49.0\n19.5\n210.0\n\n\n2\n50.0\n15.2\n218.0\n\n\n3\n45.8\n14.6\n210.0\n\n\n4\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\n\n\nI’ve learned that the top 3 variables this method will pick are 3 quantitative variables. One of the restrictions of this assignment are that one of the three variables used must be qualitative. So in the next run, I will split the columns into quantitative and qualitative, and pick the top quantitative variables and the top qualitative variables\n\n\n\nRunning it all again with more variables and separating quantitative and qualitative variables\n\n# redefining the prepare_data function to remove the Unnamed: 0 column. Also, including the egg laying and region data\ndef prepare_data1(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Comments\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  # assigning dummy values the \"int\" data type\n  df = pd.get_dummies(df,dtype = int)\n  return df, y\n\n# recreating the training data\nX_train1, y_train1 = prepare_data1(train)\n\n#discerining the quantitative and qualitative variables\n# dummy variables represent qualitative values. Since they are \"int\" data types, \n# and other quantiative variables are \"float\", they can be separated\nall_qual_cols = X_train1.select_dtypes(exclude=float)\nall_quant_cols = X_train1.select_dtypes(include=float)\n\nX_train1[all_qual_cols.columns]\n\n\n\n\n\n\n\n\nRegion_Anvers\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nDate Egg_11/10/07\nDate Egg_11/10/08\nDate Egg_11/10/09\n...\nDate Egg_11/6/08\nDate Egg_11/7/08\nDate Egg_11/8/08\nDate Egg_11/9/07\nDate Egg_11/9/08\nDate Egg_11/9/09\nDate Egg_12/1/09\nDate Egg_12/3/07\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n4\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n271\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n272\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n273\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n274\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n\n\n256 rows × 58 columns\n\n\n\nThere are significantly more qualitative variables than quantitative, but a lot of them are a binary regarding whether or not the observed penguin laid eggs on a given date.\n\nNow that I have quantitative and qualitiative variables split, I will proceed with the following feature chosing methods having them separated.\nNote: The “Date Egg” and “Island” columns are currently stored wide. If I have time, I should come back and pivot them to be long.\n\n\n# removing low threshold variance variables from the qual columns\nthresh30ql = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30ql = thresh30ql.fit_transform(X_train1[all_qual_cols.columns])\n# using univariate feature selection, using F-tests to select the best qual feature\nX_newql = SelectKBest(f_classif, k = 3)\nX_newql.fit(X_train1[all_qual_cols.columns], y_train1)\n\n# removing low threshold variance variables from the quant columns\nthresh30qt = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30qt = thresh30qt.fit_transform(X_train1[all_quant_cols.columns])\n# using univariate feature selection, using F-tests to select the 2 best quant features\nX_newqt = SelectKBest(f_classif, k = 3)\nX_newqt.fit(X_train1[all_quant_cols.columns], y_train1)\n\n# creating a new dataframe with the selected qualitative feature\ncols = X_newql.get_support(indices = True)\nX_train_newql = X_train1[all_qual_cols.columns].iloc[:,cols]\n# adding an ID column to data frame to add the selected quant features\nX_train_newql['ID'] = X_train1.index\n\n# creating a new dataframe with the selected qualitative feature\ncols = X_newqt.get_support(indices = True)\nX_train_newqt = X_train1[all_quant_cols.columns].iloc[:,cols]\n# adding an ID column\nX_train_newqt['ID'] = X_train1.index\n\n# merging best qualitative and quantitative variabels into one data set\nX_train_new_merged = pd.merge(X_train_newql, X_train_newqt, on = 'ID')\n# removing the ID column\nX_train_new_merged = X_train_new_merged.drop(['ID'], axis = 1)\n\n# viewing the new dataframe\nX_train_new_merged.head()\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 4] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n0\n1\n0\n40.9\n16.6\n187.0\n\n\n1\n0\n1\n0\n49.0\n19.5\n210.0\n\n\n2\n1\n0\n0\n50.0\n15.2\n218.0\n\n\n3\n1\n0\n0\n45.8\n14.6\n210.0\n\n\n4\n0\n1\n0\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\n\n\nAfter using both quantiative and qualitative features, it appears that the penguins island, their culmen size, and Flipper Length (mm) are the “best” features as determined by the F-Test. With these variables, I want to visualize them to see if there are any patterns\n\nimport seaborn as sns\nsns.scatterplot(X_train_new_merged, x = 'Culmen Length (mm)', y = 'Flipper Length (mm)')\n\n\n\n\n\n\n\n\nThere seems to be a positive trend, as culmen length increases, so does flipper length. There also appear to be outliers, which will make training and testing more interesting.\n\nsns.barplot(X_train_new_merged, x = 'Island_Biscoe', y = 'Culmen Depth (mm)')\n\n\n\n\n\n\n\n\nOf the penguins that live on the island and those that don’t, the culmen depth is significantly different. This leads me to believe that there might be different breeds of penguins the live on these islands, which would make islands a very strong predictor.\n\nsns.barplot(X_train_new_merged, x = 'Island_Biscoe', y = 'Flipper Length (mm)')\n\n\n\n\n\n\n\n\nOf the penguins that live on and off the island, there is a significant difference in their flipper length. This further suggests that penguing may breed on and off the island biscoe.\nTrying another method to see if I get similar variable selection\nfrom sklearn.svm import LinearSVC from sklearn.datasets import load_iris from sklearn.feature_selection import SelectFromModel X_train1, y_train1 = load_iris(return_X__y=True) X_train1.shape lsvc = LinearSVC(C=0.01, penalty=“l1”, dual=False).fit(X_train1, y_train1) model = SelectFromModel(lsvc, prefit=True) X_new = model.transform(X_train1) X_new.shape\n\n\n\n2. Model\n\nFind three features of the data and a model trained on the features that achieve 100% testing accuracy. Must use a reproducible process to obtain the three features (code up a search to obtain them). One features qualitative, two quantitative\nI think if I learned anything taking Stat learning with Professor Lyford, it is to be very suspicious if any model tests with 100% accuracy. Maybe I’m just not that good at making models, but I have never made a model with 100% testing accuracy without cheating. Either there’s a unique identifier, a tuning parameter of the model is off, or it’s just incredibly overfit.\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_train_new_merged, y_train1)\nacc = LR.score(X_train_new_merged, y_train1)\n\nacc\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.99609375\n\n\nIt appear that using these three variables, chosen by using an F-test, the linear regression has a 99% accuracy. Narrowing down the variables from 64 to just 3 to ge 99% classification accuracy on the training data is not bad!\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(LR, X_train_new_merged, y_train1, cv = 5)\ncv_scores_LR.mean()\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.9883861236802414\n\n\n\n\n\nTime to run it on the testing data and see how the model does\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\ncols = X_train_new_merged.columns\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nWhile running the model on training data does not have 100% accuracy, the model gets 100% accuracy on the testing data. I won’t think too hard into the difference in accuracy here. It does make me think that there is maybe an outlier in my training data that doesn’t occur in the testing data. That seems pretty lucky. While getting 100% accuracy should be the goal, I’m not convinced that it is always feasible. A lot of data that we work with contains outliers and sometimes identifying those outliers may lead to an overfit model that might not perform well on new data.\n\n\n3. Evaluate:\n\n\nGraphing the decision regions\n\nX_train_new_merged.head()\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n0\n1\n0\n40.9\n16.6\n187.0\n\n\n1\n0\n1\n0\n49.0\n19.5\n210.0\n\n\n2\n1\n0\n0\n50.0\n15.2\n218.0\n\n\n3\n1\n0\n0\n45.8\n14.6\n210.0\n\n\n4\n0\n1\n0\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\nGetting errors when graphing. Need to come back and revisit this code\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[3]]\n    x1 = X[X.columns[4]]\n    x2 = X[X.columns[5]]\n    qual_features = X.columns[:3]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    grid_z = np.linspace(x2.min(),x2.max(),501)\n    xx, yy, zz = np.meshgrid(grid_x, grid_y, grid_z)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n    ZZ = zz.ravel()\n\n    for i in range(len(qual_features)):\n      XYZ = pd.DataFrame({\n          X.columns[3] : XX,\n          X.columns[4] : YY,\n          X.columns[5] : ZZ\n      })\n\n      for j in qual_features:\n        XYZ[j] = 0\n\n      XYZ[qual_features[i]] = 1\n\n      p = model.predict(XYZ)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, zz, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], x2[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train_new_merged, y_train1)\n\nValueError: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n\n\n\n\n\n\n\n\n\nAs it turned out, the island on which the penguins live turned out to be a very powerful predictor in this model, in addition to culmen size."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWiDS Conference 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Goal_Setting/goal-setting.html",
    "href": "posts/Goal_Setting/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Breanna Guo\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "posts/Goal_Setting/goal-setting.html#what-youll-learn",
    "href": "posts/Goal_Setting/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process."
  },
  {
    "objectID": "posts/Goal_Setting/goal-setting.html#what-youll-achieve",
    "href": "posts/Goal_Setting/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "goal-setting.html",
    "href": "goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Breanna Guo\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process.\nBut while these are my interests, I plan on coming into Office Hours on 2/29 to discuss options for this course. I am concerned about my spring courseload and woudl love some advice.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "goal-setting.html#what-youll-learn",
    "href": "goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process.\nBut while these are my interests, I plan on coming into Office Hours on 2/29 to discuss options for this course. I am concerned about my spring courseload and woudl love some advice."
  },
  {
    "objectID": "goal-setting.html#what-youll-achieve",
    "href": "goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "This post covers my thoughts on the reading Solving the Equation: The variables for Women’s Success in Engineering and Computing, as well as a synopsis of the talks I attended and what I learned from attending the event\nI would like to note that the WiDS 2024 event was organized by me and Professor Tang from the math and statistics department as an ongiong effort to bring more data science events to Middlebury by highlighting the incredible data science work done by women. This is an opportunity for students to learn about the interdisciplenary applications of data sciecne skills and meet peers who are also interested in data science.\nmy biggest takeaways\n\n\n\nWhy is it a problem that women are underrepresented in computing, math, and engineering? For whom is it a problem?\n\n“Half our team, we’re not even putting on the field. We’ve got to change those numbers.” Former President Barack Obama, 2014 White House Science Fair\n\nUnderrepresentation of women in computing, math and engineering fileds creates stereotypes that women are worse than men in these fields, which leads to biases in decision making processes e.g. hiring. These stereotypes also lead to a negative feedback look where women become less engaged in the fields that they are negatively stereotyped in, or they perform worse at tasks when reminded of the negative stereotypes against them in those tasks. The presence of women at all levels of the organization can create more community for women, in addition to more female role models. Having more women in these fields would help reduce the implicit gender-science bias in both men and women. These issues do not lie solely apply to women. The biases that lead to underrepresenation impact everybody. As said by former president Barack Obama, less women in the field means there’s a lot of labor missing that could be there. More minds working on computing, mathm and engineering would benefit us all.\nHow is the representation and status of women in computing today different from the 1950s and 1960s? What are some of the forces that brought on this change?\nWhich of the barriers and unequal challenges described in the section “Why So Few?” can be eroded by events that spotlight the achievement of women in STEM?\n\n\n\nDone\n\n\n\nWhat was talked about, what was learned eg. data sources, what was the result of the lecture, what are assumptions they made in their research, issues they came across\n\n\n\n\n\n\n\n\n\n\n\nInlcude: topic, primary argument, what was learned\n\n\n\n\nWhat i learned from completing this blog post, what I hope to learn next"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#abstract",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#abstract",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "This post covers my thoughts on the reading Solving the Equation: The variables for Women’s Success in Engineering and Computing, as well as a synopsis of the talks I attended and what I learned from attending the event\nI would like to note that the WiDS 2024 event was organized by me and Professor Tang from the math and statistics department as an ongiong effort to bring more data science events to Middlebury by highlighting the incredible data science work done by women. This is an opportunity for students to learn about the interdisciplenary applications of data sciecne skills and meet peers who are also interested in data science.\nmy biggest takeaways"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-1-why-spotlight-women-in-data-science",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "Why is it a problem that women are underrepresented in computing, math, and engineering? For whom is it a problem?\n\n“Half our team, we’re not even putting on the field. We’ve got to change those numbers.” Former President Barack Obama, 2014 White House Science Fair\n\nUnderrepresentation of women in computing, math and engineering fileds creates stereotypes that women are worse than men in these fields, which leads to biases in decision making processes e.g. hiring. These stereotypes also lead to a negative feedback look where women become less engaged in the fields that they are negatively stereotyped in, or they perform worse at tasks when reminded of the negative stereotypes against them in those tasks. The presence of women at all levels of the organization can create more community for women, in addition to more female role models. Having more women in these fields would help reduce the implicit gender-science bias in both men and women. These issues do not lie solely apply to women. The biases that lead to underrepresenation impact everybody. As said by former president Barack Obama, less women in the field means there’s a lot of labor missing that could be there. More minds working on computing, mathm and engineering would benefit us all.\nHow is the representation and status of women in computing today different from the 1950s and 1960s? What are some of the forces that brought on this change?\nWhich of the barriers and unequal challenges described in the section “Why So Few?” can be eroded by events that spotlight the achievement of women in STEM?"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-2-attend-wids",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-2-attend-wids",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "Done"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-3-report",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-3-report",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "What was talked about, what was learned eg. data sources, what was the result of the lecture, what are assumptions they made in their research, issues they came across\n\n\n\n\n\n\n\n\n\n\n\nInlcude: topic, primary argument, what was learned"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-4-reflection",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-4-reflection",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "What i learned from completing this blog post, what I hope to learn next"
  },
  {
    "objectID": "10-compas.html",
    "href": "10-compas.html",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Today we are going to study an extremely famous investigation into algorithmic decision-making in the sphere of criminal justice by @angwin2022machine, originally written for ProPublica in 2016. This investigation significantly accelerated the pace of research into bias and fairness in machine learning, due in combination to its simple message and publicly-available data.\nIt’s helpful to look at a sample form used for feature collection in the COMPAS risk assessment.\nYou may have already read about the COMPAS algorithm in the original article at ProPublica. Our goal today is to reproduce some of the main findings of this article and set the stage for a more systematic treatment of bias and fairness in machine learning.\nParts of these lecture notes are inspired by the original ProPublica analysis and Allen Downey’s expository case study on the same data.\n\n\n Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\nOur data now looks like this:\n\n\n\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\nWhat about race?\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?\n\n\n\n\n\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?\n\n\n\n\n\n\n@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false].\n\n\n\nIn these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates).\n\n\n\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "10-compas.html#data-preparation",
    "href": "10-compas.html#data-preparation",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\nOur data now looks like this:"
  },
  {
    "objectID": "10-compas.html#preliminary-explorations",
    "href": "10-compas.html#preliminary-explorations",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s do some quick exploration of our data. How many defendants are present in this data of each sex?\nWhat about race?\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?"
  },
  {
    "objectID": "10-compas.html#the-propublica-findings",
    "href": "10-compas.html#the-propublica-findings",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?"
  },
  {
    "objectID": "10-compas.html#the-rebuttal",
    "href": "10-compas.html#the-rebuttal",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false]."
  },
  {
    "objectID": "10-compas.html#recap",
    "href": "10-compas.html#recap",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "In these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates)."
  },
  {
    "objectID": "10-compas.html#some-questions-moving-forward",
    "href": "10-compas.html#some-questions-moving-forward",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Can we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  }
]