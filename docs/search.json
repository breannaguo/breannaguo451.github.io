[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Warmup_Feb15.html",
    "href": "Warmup_Feb15.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#Meeting the Palmer Penguins Warmup Activity Breanna Guo 2/14/24\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\n\n\ndf.columns\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\nPart A: Make a summary table. How does the mean mass of penguins vary by species and sex?\n\nmeanMass = df.groupby([\"Species\", 'Sex'])[\"Body Mass (g)\"].mean()\nmeanMass\n\nSpecies                                    Sex   \nAdelie Penguin (Pygoscelis adeliae)        FEMALE    3368.835616\n                                           MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica)  FEMALE    3527.205882\n                                           MALE      3938.970588\nGentoo penguin (Pygoscelis papua)          .         4875.000000\n                                           FEMALE    4679.741379\n                                           MALE      5484.836066\nName: Body Mass (g), dtype: float64\n\n\nPart B: Make a scatterplot of clumen length against culmen depth, with the color of each point corresponding to the penguin species\n\n#df.plot.scatter(x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species', colormap = 'jet')\n\nsns.scatterplot(df, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species')"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Penguins/Blog1Penguins.html",
    "href": "posts/Penguins/Blog1Penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n2/22/2024\n\n# Accessing the training data\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\n182\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\n192\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\n223\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\n234\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\n185\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n1. Explore\n\nConstruct two interesting displayed figure and at least one interesting displayed summary table. Include discussino of figures and table- explain what is learned about the data from the products\n\n\n\nStarting with Data Preparation:\n\nConverting “Sex” and “Island” columns into binary values.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nNext, in variable selection, to narrow the options down to just three variables, one thing that can be done is remove variables with low variance. Choosing to set a threshold at 80%, for example, removes variables where 80% or more observations share the same variable value.\n\nfrom sklearn.feature_selection import VarianceThreshold\nthresh = VarianceThreshold(threshold = (0.8*(1-0.8)))\nhighVar = thresh.fit_transform(X_train)\nhighVar.shape, X_train.shape\n\n\n\n\n\n\n\n\nUnnamed: 0\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n182\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n192\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n223\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n234\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n185\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nRemoving features with less than 20% variance got rid of 4 out of 15 variables. As variance increases, I am interested in when and where the msot variables no longer make the threshold\n\nthresh1 = VarianceThreshold(threshold = (0.5*(1-0.5)))\nhighVar1 = thresh1.fit_transform(X_train)\nhighVar1.shape, X_train.shape\n\n\n\n\n\n\n\n\nUnnamed: 0\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n182\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n192\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n223\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n234\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n185\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nAt around 50% variance, the majority of variables no longer reach the variance threshold. Cool! I will probably use a threshold around 30% variance\n\nthresh30 = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30 = thresh30.fit_transform(X_train)\nhighVar30.shape, X_train.shape\n\n((256, 11), (256, 15))\n\n\n\n\nNext, using Univariate feature selection, we can use an F-test to retreive the three best features for the dataset, estimating the degree of linear dependency between the variables.\n\n#from sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n#X_train, y_train = load_iris(return_X_y = True)\n#X_train.shape\n# Create and fit selector\nX_new = SelectKBest(f_classif, k = 3)\nX_new.fit(X_train, y_train)\n\n\n\nSelectKBest(k=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SelectKBestSelectKBest(k=3)\n\n\n\n\nNow that we have the 3 best features, we have to create a new dataframe with the columns we want in it\n\n# Get columns to keep and create new dataframe\ncols = X_new.get_support(indices = True)\nX_train_new = X_train.iloc[:,cols]\n\nX_train_new.head()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'iloc'\n\n\n\n\n\nInteresting…. so after all of that, there are three columns: Unnamed: 0, Culmen Length (mm), and Flipper Length (mm). What is unnamed:0? was that in the original data set and named as such or did the name get lost in translation?\n\ntrain.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\n182\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\n192\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\n223\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\n234\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\n185\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\nOk so Unnamed: 0 exists in the main dataset, I wonder if there are any duplicated values in Unnamed: 0\n\nX_train[X_train.duplicated(['Unnamed: 0'], keep=False)]\n\n\n\n\n\n\n\n\nUnnamed: 0\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n\n\n\n\n\n\nX_train['Unnamed: 0'], X_train['Unnamed: 0'].max(), X_train['Unnamed: 0'].min()\n\n(0      182\n 1      192\n 2      223\n 3      234\n 4      185\n       ... \n 270    319\n 271     68\n 272     32\n 273      4\n 274    172\n Name: Unnamed: 0, Length: 256, dtype: int64,\n 343,\n 2)\n\n\n\n\nThere are no duplicated values in that column. Since that is the case, I believe it is some other form of unique identifier, which will overfit any model.\n\n\nAdditionally, I’ve learned that the top 3 variables this method will pick are 3 quantitative variables. One of the restrictions of this assignment are that one of the three variables used must be qualitative. So in the next run, I will split the columns into quantitative and qualitative, and pick the top quantitative variables and the top qualitative variables\n\n\n\nRunning it all again without ‘Unnamed: 0’\n\n# redefining the prepare_data function to remove the Unnamed: 0 column. Also, including the egg laying and region data\ndef prepare_data1(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Comments\", \"Unnamed: 0\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  # assigning dummy values the \"int\" data type\n  df = pd.get_dummies(df,dtype = int)\n  return df, y\n\n# recreating the training data\nX_train1, y_train1 = prepare_data1(train)\n\n#discerining the quantitative and qualitative variables\n# dummy variables represent qualitative values. Since they are \"int\" data types, \n# and other quantiative variables are \"float\", they can be separated\nall_qual_cols = X_train1.select_dtypes(exclude=float)\nall_quant_cols = X_train1.select_dtypes(include=float)\n\nX_train1[all_qual_cols.columns]\n\n\n\n\n\n\n\n\nRegion_Anvers\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nDate Egg_11/10/07\nDate Egg_11/10/08\nDate Egg_11/10/09\n...\nDate Egg_11/6/08\nDate Egg_11/7/08\nDate Egg_11/8/08\nDate Egg_11/9/07\nDate Egg_11/9/08\nDate Egg_11/9/09\nDate Egg_12/1/09\nDate Egg_12/3/07\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n4\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n271\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n272\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n273\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n274\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n\n\n256 rows × 58 columns\n\n\n\nThere are significantly more qualitative variables than quantitative, but a lot of them are a binary regarding whether or not the observed penguin laid eggs on a given date.\n\nNow that I have quantitative and qualitiative variables split, I will proceed with the following feature chosing methods having them separated.\nNote: The “Date Egg” and “Island” columns are currently stored wide. If I have time, I should come back and pivot them to be long.\n\n\n# removing low threshold variance variables from the qual columns\nthresh30ql = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30ql = thresh30ql.fit_transform(X_train1[all_qual_cols.columns])\n# using univariate feature selection, using F-tests to select the best qual feature\nX_newql = SelectKBest(f_classif, k = 3)\nX_newql.fit(X_train1[all_qual_cols.columns], y_train1)\n\n# removing low threshold variance variables from the quant columns\nthresh30qt = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30qt = thresh30qt.fit_transform(X_train1[all_quant_cols.columns])\n# using univariate feature selection, using F-tests to select the 2 best quant features\nX_newqt = SelectKBest(f_classif, k = 3)\nX_newqt.fit(X_train1[all_quant_cols.columns], y_train1)\n\n# creating a new dataframe with the selected qualitative feature\ncols = X_newql.get_support(indices = True)\nX_train_newql = X_train1[all_qual_cols.columns].iloc[:,cols]\n# adding an ID column to data frame to add the selected quant features\nX_train_newql['ID'] = X_train1.index\n\n# creating a new dataframe with the selected qualitative feature\ncols = X_newqt.get_support(indices = True)\nX_train_newqt = X_train1[all_quant_cols.columns].iloc[:,cols]\n# adding an ID column\nX_train_newqt['ID'] = X_train1.index\n\n# merging best qualitative and quantitative variabels into one data set\nX_train_new_merged = pd.merge(X_train_newql, X_train_newqt, on = 'ID')\n# removing the ID column\nX_train_new_merged = X_train_new_merged.drop(['ID'], axis = 1)\n\n# viewing the new dataframe\nX_train_new_merged.head()\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 4] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n0\n1\n0\n40.9\n16.6\n187.0\n\n\n1\n0\n1\n0\n49.0\n19.5\n210.0\n\n\n2\n1\n0\n0\n50.0\n15.2\n218.0\n\n\n3\n1\n0\n0\n45.8\n14.6\n210.0\n\n\n4\n0\n1\n0\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\n\n\nAfter using both quantiative and qualitative features, it appears that the penguins island, their culmen size, and Flipper Length (mm) are the “best” features as determined by the F-Test. With these variables, I want to visualize them to see if there are any patterns\n\nimport seaborn as sns\nsns.scatterplot(X_train_new_merged, x = 'Culmen Length (mm)', y = 'Flipper Length (mm)')\n\n\n\n\n\n\n\n\nThere seems to be a positive trend, as culmen length increases, so does flipper length. There also appear to be outliers, which will make training and testing more interesting.\n\nsns.barplot(X_train_new_merged, x = 'Island_Biscoe', y = 'Culmen Depth (mm)')\n\n\n\n\n\n\n\n\nOf the penguins that live on the island and those that don’t, the culmen length is not significantly different\n\nsns.barplot(X_train_new_merged, x = 'Island_Biscoe', y = 'Flipper Length (mm)')\n\n\n\n\n\n\n\n\nOf the penguins that live on and off the island, there is a significant difference in their flipper length.\nTrying another method to see if I get similar variable selection\nfrom sklearn.svm import LinearSVC from sklearn.datasets import load_iris from sklearn.feature_selection import SelectFromModel X_train1, y_train1 = load_iris(return_X__y=True) X_train1.shape lsvc = LinearSVC(C=0.01, penalty=“l1”, dual=False).fit(X_train1, y_train1) model = SelectFromModel(lsvc, prefit=True) X_new = model.transform(X_train1) X_new.shape\n\n\n\n2. Model\n\nFind three features of the data and a model trained on the features that achieve 100% testing accuracy. Must use a reproducible process to obtain the three features (code up a search to obtain them). One features qualitative, two quantitative\nI think if I learned anything taking Stat learning with Professor Lyford, it is to be very suspicious if any model tests with 100% accuracy. Maybe I’m just not that good at making models, but I have never made a model with 100% testing accuracy without cheating. Either there’s a unique identifier, a tuning parameter of the model is off, or it’s just incredibly overfit.\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_train_new_merged, y_train1)\nacc = LR.score(X_train_new_merged, y_train1)\n\nacc\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.99609375\n\n\nIt appear that using these three variables, chosen by using an F-test, the linear regression has a 99% accuracy. Narrowing down the variables from 64 to just 3 to ge 99% classification accuracy on the training data is not bad!\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(LR, X_train_new_merged, y_train1, cv = 5)\ncv_scores_LR.mean()\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.9883861236802414\n\n\n\n\n\nTime to run it on the testing data and see how the model does\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\ncols = X_train_new_merged.columns\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nWhile running the model on training data does not have 100% accuracy, the model gets 100% accuracy on the testing data. I won’t think too hard into the difference in accuracy here. It does make me think that there is maybe an outlier in my training data that doesn’t occur in the testing data\n\n\nGraphing the decision regions\n\nX_train_new_merged.head()\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n0\n1\n0\n40.9\n16.6\n187.0\n\n\n1\n0\n1\n0\n49.0\n19.5\n210.0\n\n\n2\n1\n0\n0\n50.0\n15.2\n218.0\n\n\n3\n1\n0\n0\n45.8\n14.6\n210.0\n\n\n4\n0\n1\n0\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[3]]\n    x1 = X[X.columns[4]]\n    x2 = X[X.columns[5]]\n    qual_features = X.columns[:3]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    grid_z = np.linspace(x2.min(),x2.max(),501)\n    xx, yy, zz = np.meshgrid(grid_x, grid_y, grid_z)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n    ZZ = zz.ravel()\n\n    for i in range(len(qual_features)):\n      XYZ = pd.DataFrame({\n          X.columns[3] : XX,\n          X.columns[4] : YY,\n          X.columns[5] : ZZ\n      })\n\n      for j in qual_features:\n        XYZ[j] = 0\n\n      XYZ[qual_features[i]] = 1\n\n      p = model.predict(XYZ)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, zz, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], x2[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train_new_merged, y_train1)\n\nValueError: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n\n\n\n\n\n\n\n\n\n\n\n3. Evaluate:\n\nShow decision regions of mode, split by qualitative feature"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]