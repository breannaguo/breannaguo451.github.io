[
  {
    "objectID": "goal-setting.html",
    "href": "goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Breanna Guo\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process.\nBut while these are my interests, I plan on coming into Office Hours on 2/29 to discuss options for this course. I am concerned about my spring courseload and woudl love some advice.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "goal-setting.html#what-youll-learn",
    "href": "goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process.\nBut while these are my interests, I plan on coming into Office Hours on 2/29 to discuss options for this course. I am concerned about my spring courseload and woudl love some advice."
  },
  {
    "objectID": "goal-setting.html#what-youll-achieve",
    "href": "goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "10-compas.html",
    "href": "10-compas.html",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Today we are going to study an extremely famous investigation into algorithmic decision-making in the sphere of criminal justice by @angwin2022machine, originally written for ProPublica in 2016. This investigation significantly accelerated the pace of research into bias and fairness in machine learning, due in combination to its simple message and publicly-available data.\nIt’s helpful to look at a sample form used for feature collection in the COMPAS risk assessment.\nYou may have already read about the COMPAS algorithm in the original article at ProPublica. Our goal today is to reproduce some of the main findings of this article and set the stage for a more systematic treatment of bias and fairness in machine learning.\nParts of these lecture notes are inspired by the original ProPublica analysis and Allen Downey’s expository case study on the same data.\n\n\n Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\nOur data now looks like this:\n\n\n\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\nWhat about race?\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?\n\n\n\n\n\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?\n\n\n\n\n\n\n@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false].\n\n\n\nIn these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates).\n\n\n\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "10-compas.html#data-preparation",
    "href": "10-compas.html#data-preparation",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\nOur data now looks like this:"
  },
  {
    "objectID": "10-compas.html#preliminary-explorations",
    "href": "10-compas.html#preliminary-explorations",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s do some quick exploration of our data. How many defendants are present in this data of each sex?\nWhat about race?\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?"
  },
  {
    "objectID": "10-compas.html#the-propublica-findings",
    "href": "10-compas.html#the-propublica-findings",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?"
  },
  {
    "objectID": "10-compas.html#the-rebuttal",
    "href": "10-compas.html#the-rebuttal",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false]."
  },
  {
    "objectID": "10-compas.html#recap",
    "href": "10-compas.html#recap",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "In these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates)."
  },
  {
    "objectID": "10-compas.html#some-questions-moving-forward",
    "href": "10-compas.html#some-questions-moving-forward",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Can we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "Warmup_Feb15.html",
    "href": "Warmup_Feb15.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#Meeting the Palmer Penguins Warmup Activity Breanna Guo 2/14/24\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\n\n\ndf.columns\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\nPart A: Make a summary table. How does the mean mass of penguins vary by species and sex?\n\nmeanMass = df.groupby([\"Species\", 'Sex'])[\"Body Mass (g)\"].mean()\nmeanMass\n\nSpecies                                    Sex   \nAdelie Penguin (Pygoscelis adeliae)        FEMALE    3368.835616\n                                           MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica)  FEMALE    3527.205882\n                                           MALE      3938.970588\nGentoo penguin (Pygoscelis papua)          .         4875.000000\n                                           FEMALE    4679.741379\n                                           MALE      5484.836066\nName: Body Mass (g), dtype: float64\n\n\nPart B: Make a scatterplot of clumen length against culmen depth, with the color of each point corresponding to the penguin species\n\n#df.plot.scatter(x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species', colormap = 'jet')\n\nsns.scatterplot(df, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species')"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/MidCourse_Goals/mid-course.html",
    "href": "posts/MidCourse_Goals/mid-course.html",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "Breanna Guo\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) I have attended every class except one, when I attended a conference.\nHow often have you taken notes on the core readings ahead of the class period? I do the readings, I don’t necessarily take notes unless for personal reasons.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? I have most always been prepared to engage in the warm-up exercise.\nHow many times have you actually presented the daily warm-up to your team? I’ve lead a couple of times, and we’ve done group discussions a number of times as well when I wasn’t leader.\nHow many times have you asked your team for help while presenting the daily warm-up? I have had questions about the warmup when presenting.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Almost every class\nHow often have you helped a teammate during the daily warm-up presentation? Once or twice. We will often discuss how we approached the question.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? Once or twice\nHow often have you asked for or received help from your fellow students? Often, for warmups and blog posts\nHave you been regularly participating in a study group outside class? No\nHow often have you posted questions or answers in Slack? Not at all\n\n\n\n\n\nHow many blog posts have you submitted? 2\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 1\nM: Revisions useful: 0\nR: Revisions encouraged: 1\nN: Incomplete: 1\n\nRoughly how many hours per week have you spent on this course outside of class? 2-3\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI spent a lot of time on the social responsibility category of this class, mainly due to my involvement with WiDS, but also because I found learning about it really fascinating. I’ve found myself doing additional readings outside of class and taking on an project in another class using the COMPAS dataset to further explore potential biases in the algorithm.\nI am quite interested in the theory element because I see so many parallels and applications while taking Multi variable calculus right now. I hope to do a blog post on logistic regression or some other method soon.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it. When I made my goals at the start of this semester, I was about one day out from talking to Prof. Phil about taking this course credit/no credit. I made those goals without thinking too hard because I knew I would be reassessing them very soon. I’d say my goals from the start of the semester were:\n\nCompleting 4 Blog posts, one on each of the categories\n\nThis, I am halfway through at this point, and as we are halfway throught the semester that seems about right.\n\nComing prepared to each class by completing the warmup\n\nI’ve done all the warmups to the best of my ability so far, so this goal seems well on track\n\nParticipating in class\n\nI think I’ve been doing alright participation-wise. I definitely don’t ask all the questions that I have so I could be better about that. But in terms of group participation, I have been participating well in the warmup activity discussions *Doing the Final Project\nThis hasn’t happened yet but I’m thinking of a topic that I’m interested in which is a good start.\n\n\n\nRegarding blog posts, I feel on track, but I would like to check in with Prof. Phil about which blog posts I want to complete in the future. I feel the best about my warmup and class participation so far.\n\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal. Mainly, I would like to ask more questions in class. I have definitely felt lost at some points when talking through the math methods and therefore lost the thread, so being more proactive when I start to feel lost would be a good way for me to enhance my understanding of the course material.\n\nI also haven’t engaged too heavily in any study group. I think part of me taking this class credit no credit means I haven’t needed to engage is too many outside of class activities. I do see myself using study groups more heavily, especially my final project group when we start working on the projects.\n\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\nI’m feeling good about where I am relative to my goals, but I would like to check in with Prof. Phil again soon to discuss my trajectory for the next half of this course. March was the data collecting and cleaning portion of my econ thesis and I really had to clean that data. I’m getting to a point where the data is usable, and once it’s usable, all I have to do it type of some lines of code to run the regressions. That means I will hopefully have more time to give to my other classes, and work on the final project and blog posts\n\n\nI am halfway through completing my goal of 4 blog posts at this point, and as we are halfway throught the semester that seems about right. I would like to check in with Prof. Phil about which blog posts I want to complete in the future. I feel the best about my warmup and class participation so far.\n\n\n\nI think I’ve been doing alright participation-wise. I definitely don’t ask all the questions that I have so I could be better about that. But in terms of group participation, I have been participating well in the warmup activity discussions\n\n\n\nI haven’t started on my final project yet, but I hope to brainstorm ideas for it.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI have really enjoyed learning about bias in Machine Learning, it is somehing I’ve never thought critically about before. Knowing now, how important it is to question the conclusions made by algorithms, I want to learn more about how to assess and notice when biases may be present.\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nGoals:\n* Completing 4 Blog posts, one on each of the categories\n    - I have completed 2 posts so far.\n    - I don't have any planned revisions so far\n* Coming prepared to each class by completing the warmup\n    - I've done all the warmups to the best of my ability so far\n    - I don't have any planned revisions so far\n* Participating in class\n    - I think I've been doing alright participation-wise. I definitely don't ask all the questions that I have so I could be better about that. But in terms of group participation, I have been participating well in the warmup activity discussions\n    - I don't have any revisions to this goal- I just want to participate more to meet this goal\n*Doing the Final Project\n    - This hasn't happened yet but I'm thinking of a topic that I'm interested in which is a good start.\n    - No revisions a the moment, I just want to use ML on a dataset that interests me.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of B\n\n\nA way in which I resonate with the soundbytes for that grade above is… ” I see a few ideas… that will be relevant for future classes…” and “I am able to … achieve new tasks” with these new skills. I also do see opportunities to learn more- e.g. blog posts- but overall, I feel good about my time in this course.\n\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nAs previously stated, I want to check in with Prof. Phil about where I’m at and how to proceed in the next couple of weeks. At the start of the semester I said I wanted to consider taking this course C/NC and given the amount of work and effort I’ve put into the course so far, I think that is my best option.\nI overall feel like I have learned so much in this course. So many parts of my life outside of this course- WiDS, Multivariable calculus- have ties to this course and have lead me to be more engaged and interested in topics that I may not have thought would interest me."
  },
  {
    "objectID": "posts/MidCourse_Goals/mid-course.html#the-data",
    "href": "posts/MidCourse_Goals/mid-course.html#the-data",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "In this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) I have attended every class except one, when I attended a conference.\nHow often have you taken notes on the core readings ahead of the class period? I do the readings, I don’t necessarily take notes unless for personal reasons.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? I have most always been prepared to engage in the warm-up exercise.\nHow many times have you actually presented the daily warm-up to your team? I’ve lead a couple of times, and we’ve done group discussions a number of times as well when I wasn’t leader.\nHow many times have you asked your team for help while presenting the daily warm-up? I have had questions about the warmup when presenting.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Almost every class\nHow often have you helped a teammate during the daily warm-up presentation? Once or twice. We will often discuss how we approached the question.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? Once or twice\nHow often have you asked for or received help from your fellow students? Often, for warmups and blog posts\nHave you been regularly participating in a study group outside class? No\nHow often have you posted questions or answers in Slack? Not at all\n\n\n\n\n\nHow many blog posts have you submitted? 2\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 1\nM: Revisions useful: 0\nR: Revisions encouraged: 1\nN: Incomplete: 1\n\nRoughly how many hours per week have you spent on this course outside of class? 2-3"
  },
  {
    "objectID": "posts/MidCourse_Goals/mid-course.html#what-youve-learned",
    "href": "posts/MidCourse_Goals/mid-course.html#what-youve-learned",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "At the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI spent a lot of time on the social responsibility category of this class, mainly due to my involvement with WiDS, but also because I found learning about it really fascinating. I’ve found myself doing additional readings outside of class and taking on an project in another class using the COMPAS dataset to further explore potential biases in the algorithm.\nI am quite interested in the theory element because I see so many parallels and applications while taking Multi variable calculus right now. I hope to do a blog post on logistic regression or some other method soon."
  },
  {
    "objectID": "posts/MidCourse_Goals/mid-course.html#reflecting-on-goals",
    "href": "posts/MidCourse_Goals/mid-course.html#reflecting-on-goals",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "For each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it. When I made my goals at the start of this semester, I was about one day out from talking to Prof. Phil about taking this course credit/no credit. I made those goals without thinking too hard because I knew I would be reassessing them very soon. I’d say my goals from the start of the semester were:\n\nCompleting 4 Blog posts, one on each of the categories\n\nThis, I am halfway through at this point, and as we are halfway throught the semester that seems about right.\n\nComing prepared to each class by completing the warmup\n\nI’ve done all the warmups to the best of my ability so far, so this goal seems well on track\n\nParticipating in class\n\nI think I’ve been doing alright participation-wise. I definitely don’t ask all the questions that I have so I could be better about that. But in terms of group participation, I have been participating well in the warmup activity discussions *Doing the Final Project\nThis hasn’t happened yet but I’m thinking of a topic that I’m interested in which is a good start.\n\n\n\nRegarding blog posts, I feel on track, but I would like to check in with Prof. Phil about which blog posts I want to complete in the future. I feel the best about my warmup and class participation so far.\n\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal. Mainly, I would like to ask more questions in class. I have definitely felt lost at some points when talking through the math methods and therefore lost the thread, so being more proactive when I start to feel lost would be a good way for me to enhance my understanding of the course material.\n\nI also haven’t engaged too heavily in any study group. I think part of me taking this class credit no credit means I haven’t needed to engage is too many outside of class activities. I do see myself using study groups more heavily, especially my final project group when we start working on the projects.\n\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\nI’m feeling good about where I am relative to my goals, but I would like to check in with Prof. Phil again soon to discuss my trajectory for the next half of this course. March was the data collecting and cleaning portion of my econ thesis and I really had to clean that data. I’m getting to a point where the data is usable, and once it’s usable, all I have to do it type of some lines of code to run the regressions. That means I will hopefully have more time to give to my other classes, and work on the final project and blog posts\n\n\nI am halfway through completing my goal of 4 blog posts at this point, and as we are halfway throught the semester that seems about right. I would like to check in with Prof. Phil about which blog posts I want to complete in the future. I feel the best about my warmup and class participation so far.\n\n\n\nI think I’ve been doing alright participation-wise. I definitely don’t ask all the questions that I have so I could be better about that. But in terms of group participation, I have been participating well in the warmup activity discussions\n\n\n\nI haven’t started on my final project yet, but I hope to brainstorm ideas for it.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI have really enjoyed learning about bias in Machine Learning, it is somehing I’ve never thought critically about before. Knowing now, how important it is to question the conclusions made by algorithms, I want to learn more about how to assess and notice when biases may be present.\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nGoals:\n* Completing 4 Blog posts, one on each of the categories\n    - I have completed 2 posts so far.\n    - I don't have any planned revisions so far\n* Coming prepared to each class by completing the warmup\n    - I've done all the warmups to the best of my ability so far\n    - I don't have any planned revisions so far\n* Participating in class\n    - I think I've been doing alright participation-wise. I definitely don't ask all the questions that I have so I could be better about that. But in terms of group participation, I have been participating well in the warmup activity discussions\n    - I don't have any revisions to this goal- I just want to participate more to meet this goal\n*Doing the Final Project\n    - This hasn't happened yet but I'm thinking of a topic that I'm interested in which is a good start.\n    - No revisions a the moment, I just want to use ML on a dataset that interests me."
  },
  {
    "objectID": "posts/MidCourse_Goals/mid-course.html#grade-and-goals",
    "href": "posts/MidCourse_Goals/mid-course.html#grade-and-goals",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "Take 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of B\n\n\nA way in which I resonate with the soundbytes for that grade above is… ” I see a few ideas… that will be relevant for future classes…” and “I am able to … achieve new tasks” with these new skills. I also do see opportunities to learn more- e.g. blog posts- but overall, I feel good about my time in this course."
  },
  {
    "objectID": "posts/MidCourse_Goals/mid-course.html#optional-how-to-improve",
    "href": "posts/MidCourse_Goals/mid-course.html#optional-how-to-improve",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "You may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nAs previously stated, I want to check in with Prof. Phil about where I’m at and how to proceed in the next couple of weeks. At the start of the semester I said I wanted to consider taking this course C/NC and given the amount of work and effort I’ve put into the course so far, I think that is my best option.\nI overall feel like I have learned so much in this course. So many parts of my life outside of this course- WiDS, Multivariable calculus- have ties to this course and have lead me to be more engaged and interested in topics that I may not have thought would interest me."
  },
  {
    "objectID": "posts/WiDS Blog Post/Untitled-1.html",
    "href": "posts/WiDS Blog Post/Untitled-1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#WiDS 24 Stanford conference\n** What I learned and who I met\nKelly Stroh- works at a pet insurance company discussed potential bias in the ML algorithems used to determine insurance policy when it comes to pet nicknames from other languages that the algorithm might not be able to discern.\nS- WiDS ambassador from Tel Aviv but works at Intuit, specifically working with the algorithms that read in information from tax forms (Turbo tax)\nMariana- WiDS ambassador from Lima Peru, doing her undergrad thesis on the Peruvian sign language which involves collecting video data of all of the alphabet signs and then programming them\nEllen Pao- former Kleiner Perkings employee who sued them for discrimmination, became interim Reddit CEO and now has started Project Interlude\nEmily Gordon- Stanford Post-doc, we can create models of the worlds climate systems to isolate the effect of CO2 and other agents on global warming"
  },
  {
    "objectID": "posts/Goal_Setting/goal-setting.html",
    "href": "posts/Goal_Setting/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Breanna Guo\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "posts/Goal_Setting/goal-setting.html#what-youll-learn",
    "href": "posts/Goal_Setting/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am very interested in the theory and the experimentation part of this syllabus. I am currently in multivariable calculus and I previously took linear algebra, which both feel very applicable in this class and I love applying things learned in outside courses to my current courses.\nI also think that assessing algorithm perfomance is something much more nuanced than we usually assume and it’s something I find fascinating. Additionally, being able to effectively communicate results is something I find extremely important to the research process."
  },
  {
    "objectID": "posts/Goal_Setting/goal-setting.html#what-youll-achieve",
    "href": "posts/Goal_Setting/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nI would like to enhance my knowledge of social responsibility and bias in models and to do so I intend to complete two blog posts within that learning objective.\nFor the blog posts that I choose to turn in, I want to put aside the need for it to be perfect the first time around try to turn them in on time (with the exception of the first one where I was confused with Git)\nI would be interested in proposing and completeing a blog post on a topic not covered in the lecture- specifically one that interests me eg. machine learning with spatial data\nI might be interested in doing a blog post after reading a book on bias in ML- could discuss more with Prof. Phil if this is reasonable\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nI intend to complete the warmups before each class\nI would like to skim at least all of the reading and read/ explore further literature on topics of interest\nI would like to get closer with acquaintences in the course\nIf I get chosen to be the team leader, I would like to be a good discussion leader\nIf there are guest speakers, I would love to read about them beforehand and have questions prepared.\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nI would like to have good communication with my project partners\nI would like to delegate tasks appropriately amongst my project partners\nI would like to submit project milestones on time\nIt would be nice to have a project that I am interested in- that would definitely make me more engaged with the project"
  },
  {
    "objectID": "posts/Penguins/Blog1Penguins.html",
    "href": "posts/Penguins/Blog1Penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n2/22/2024\n\n# Accessing the training data\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n1. Explore\n\nConstruct two interesting displayed figure and at least one interesting displayed summary table. Include discussino of figures and table- explain what is learned about the data from the products\n\n\n\nStarting with Data Preparation:\n\nConverting “Sex” and “Island” columns into binary values.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nNext, in variable selection, to narrow the options down to just three variables, one thing that can be done is remove variables with low variance. Choosing to set a threshold at 80%, for example, removes variables where 80% or more observations share the same variable value.\n\nfrom sklearn.feature_selection import VarianceThreshold\nthresh = VarianceThreshold(threshold = (0.8*(1-0.8)))\nhighVar = thresh.fit_transform(X_train)\nhighVar.shape, X_train.shape\n\n((256, 10), (256, 14))\n\n\n\n\nRemoving features with less than 20% variance got rid of 4 out of 15 variables. As variance increases, I am interested in when and where the msot variables no longer make the threshold\n\nthresh1 = VarianceThreshold(threshold = (0.5*(1-0.5)))\nhighVar1 = thresh1.fit_transform(X_train)\nhighVar1.shape, X_train.shape\n\n((256, 6), (256, 14))\n\n\n\n\nAt around 50% variance, the majority of variables no longer reach the variance threshold. Cool! I will probably use a threshold around 30% variance\n\nthresh30 = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30 = thresh30.fit_transform(X_train)\nhighVar30.shape, X_train.shape\n\n((256, 10), (256, 14))\n\n\n\n\nNext, using Univariate feature selection, we can use an F-test to retreive the three best features for the dataset, estimating the degree of linear dependency between the variables.\n\n#from sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n#X_train, y_train = load_iris(return_X_y = True)\n#X_train.shape\n# Create and fit selector\nX_new = SelectKBest(f_classif, k = 3)\nX_new.fit(X_train, y_train)\n\n\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [9] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\nSelectKBest(k=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SelectKBestSelectKBest(k=3)\n\n\n\n\nNow that we have the 3 best features, we have to create a new dataframe with the columns we want in it\n\n# Get columns to keep and create new dataframe\ncols = X_new.get_support(indices = True)\nX_train_new = X_train.iloc[:,cols]\n\nX_train_new.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n40.9\n16.6\n187.0\n\n\n1\n49.0\n19.5\n210.0\n\n\n2\n50.0\n15.2\n218.0\n\n\n3\n45.8\n14.6\n210.0\n\n\n4\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\n\n\nI’ve learned that the top 3 variables this method will pick are 3 quantitative variables. One of the restrictions of this assignment are that one of the three variables used must be qualitative. So in the next run, I will split the columns into quantitative and qualitative, and pick the top quantitative variables and the top qualitative variables\n\n\n\nRunning it all again with more variables and separating quantitative and qualitative variables\n\n# redefining the prepare_data function to remove the Unnamed: 0 column. Also, including the egg laying and region data\ndef prepare_data1(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Comments\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  # assigning dummy values the \"int\" data type\n  df = pd.get_dummies(df,dtype = int)\n  return df, y\n\n# recreating the training data\nX_train1, y_train1 = prepare_data1(train)\n\n#discerining the quantitative and qualitative variables\n# dummy variables represent qualitative values. Since they are \"int\" data types, \n# and other quantiative variables are \"float\", they can be separated\nall_qual_cols = X_train1.select_dtypes(exclude=float)\nall_quant_cols = X_train1.select_dtypes(include=float)\n\nX_train1[all_qual_cols.columns]\n\n\n\n\n\n\n\n\nRegion_Anvers\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nDate Egg_11/10/07\nDate Egg_11/10/08\nDate Egg_11/10/09\n...\nDate Egg_11/6/08\nDate Egg_11/7/08\nDate Egg_11/8/08\nDate Egg_11/9/07\nDate Egg_11/9/08\nDate Egg_11/9/09\nDate Egg_12/1/09\nDate Egg_12/3/07\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n4\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n271\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n272\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n273\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n274\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n\n\n256 rows × 58 columns\n\n\n\nThere are significantly more qualitative variables than quantitative, but a lot of them are a binary regarding whether or not the observed penguin laid eggs on a given date.\n\nNow that I have quantitative and qualitiative variables split, I will proceed with the following feature chosing methods having them separated.\nNote: The “Date Egg” and “Island” columns are currently stored wide. If I have time, I should come back and pivot them to be long.\n\n\n# removing low threshold variance variables from the qual columns\nthresh30ql = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30ql = thresh30ql.fit_transform(X_train1[all_qual_cols.columns])\n# using univariate feature selection, using F-tests to select the best qual feature\nX_newql = SelectKBest(f_classif, k = 3)\nX_newql.fit(X_train1[all_qual_cols.columns], y_train1)\n\n# removing low threshold variance variables from the quant columns\nthresh30qt = VarianceThreshold(threshold = (0.7*(1-0.7)))\nhighVar30qt = thresh30qt.fit_transform(X_train1[all_quant_cols.columns])\n# using univariate feature selection, using F-tests to select the 2 best quant features\nX_newqt = SelectKBest(f_classif, k = 3)\nX_newqt.fit(X_train1[all_quant_cols.columns], y_train1)\n\n# creating a new dataframe with the selected qualitative feature\ncols = X_newql.get_support(indices = True)\nX_train_newql = X_train1[all_qual_cols.columns].iloc[:,cols]\n# adding an ID column to data frame to add the selected quant features\nX_train_newql['ID'] = X_train1.index\n\n# creating a new dataframe with the selected qualitative feature\ncols = X_newqt.get_support(indices = True)\nX_train_newqt = X_train1[all_quant_cols.columns].iloc[:,cols]\n# adding an ID column\nX_train_newqt['ID'] = X_train1.index\n\n# merging best qualitative and quantitative variabels into one data set\nX_train_new_merged = pd.merge(X_train_newql, X_train_newqt, on = 'ID')\n# removing the ID column\nX_train_new_merged = X_train_new_merged.drop(['ID'], axis = 1)\n\n# viewing the new dataframe\nX_train_new_merged.head()\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 4] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n0\n1\n0\n40.9\n16.6\n187.0\n\n\n1\n0\n1\n0\n49.0\n19.5\n210.0\n\n\n2\n1\n0\n0\n50.0\n15.2\n218.0\n\n\n3\n1\n0\n0\n45.8\n14.6\n210.0\n\n\n4\n0\n1\n0\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\n\n\nAfter using both quantiative and qualitative features, it appears that the penguins island, their culmen size, and Flipper Length (mm) are the “best” features as determined by the F-Test. With these variables, I want to visualize them to see if there are any patterns\n\nimport seaborn as sns\nsns.scatterplot(X_train_new_merged, x = 'Culmen Length (mm)', y = 'Flipper Length (mm)')\n\n\n\n\n\n\n\n\nThere seems to be a positive trend, as culmen length increases, so does flipper length. There also appear to be outliers, which will make training and testing more interesting.\n\nsns.barplot(X_train_new_merged, x = 'Island_Biscoe', y = 'Culmen Depth (mm)')\n\n\n\n\n\n\n\n\nOf the penguins that live on the island and those that don’t, the culmen depth is significantly different. This leads me to believe that there might be different breeds of penguins the live on these islands, which would make islands a very strong predictor.\n\nsns.barplot(X_train_new_merged, x = 'Island_Biscoe', y = 'Flipper Length (mm)')\n\n\n\n\n\n\n\n\nOf the penguins that live on and off the island, there is a significant difference in their flipper length. This further suggests that penguing may breed on and off the island biscoe.\nTrying another method to see if I get similar variable selection\nfrom sklearn.svm import LinearSVC from sklearn.datasets import load_iris from sklearn.feature_selection import SelectFromModel X_train1, y_train1 = load_iris(return_X__y=True) X_train1.shape lsvc = LinearSVC(C=0.01, penalty=“l1”, dual=False).fit(X_train1, y_train1) model = SelectFromModel(lsvc, prefit=True) X_new = model.transform(X_train1) X_new.shape\n\n\n\n2. Model\n\nFind three features of the data and a model trained on the features that achieve 100% testing accuracy. Must use a reproducible process to obtain the three features (code up a search to obtain them). One features qualitative, two quantitative\nI think if I learned anything taking Stat learning with Professor Lyford, it is to be very suspicious if any model tests with 100% accuracy. Maybe I’m just not that good at making models, but I have never made a model with 100% testing accuracy without cheating. Either there’s a unique identifier, a tuning parameter of the model is off, or it’s just incredibly overfit.\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_train_new_merged, y_train1)\nacc = LR.score(X_train_new_merged, y_train1)\n\nacc\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.99609375\n\n\nIt appear that using these three variables, chosen by using an F-test, the linear regression has a 99% accuracy. Narrowing down the variables from 64 to just 3 to ge 99% classification accuracy on the training data is not bad!\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(LR, X_train_new_merged, y_train1, cv = 5)\ncv_scores_LR.mean()\n\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/breannaguo/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.9883861236802414\n\n\n\n\n\nTime to run it on the testing data and see how the model does\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\ncols = X_train_new_merged.columns\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nWhile running the model on training data does not have 100% accuracy, the model gets 100% accuracy on the testing data. I won’t think too hard into the difference in accuracy here. It does make me think that there is maybe an outlier in my training data that doesn’t occur in the testing data. That seems pretty lucky. While getting 100% accuracy should be the goal, I’m not convinced that it is always feasible. A lot of data that we work with contains outliers and sometimes identifying those outliers may lead to an overfit model that might not perform well on new data.\n\n\n3. Evaluate:\n\n\nGraphing the decision regions\n\nX_train_new_merged.head()\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\n0\n0\n1\n0\n40.9\n16.6\n187.0\n\n\n1\n0\n1\n0\n49.0\n19.5\n210.0\n\n\n2\n1\n0\n0\n50.0\n15.2\n218.0\n\n\n3\n1\n0\n0\n45.8\n14.6\n210.0\n\n\n4\n0\n1\n0\n51.0\n18.8\n203.0\n\n\n\n\n\n\n\nGetting errors when graphing. Need to come back and revisit this code\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[3]]\n    x1 = X[X.columns[4]]\n    x2 = X[X.columns[5]]\n    qual_features = X.columns[:3]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    grid_z = np.linspace(x2.min(),x2.max(),501)\n    xx, yy, zz = np.meshgrid(grid_x, grid_y, grid_z)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n    ZZ = zz.ravel()\n\n    for i in range(len(qual_features)):\n      XYZ = pd.DataFrame({\n          X.columns[3] : XX,\n          X.columns[4] : YY,\n          X.columns[5] : ZZ\n      })\n\n      for j in qual_features:\n        XYZ[j] = 0\n\n      XYZ[qual_features[i]] = 1\n\n      p = model.predict(XYZ)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, zz, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], x2[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train_new_merged, y_train1)\n\nValueError: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n\n\n\n\n\n\n\n\n\nAs it turned out, the island on which the penguins live turned out to be a very powerful predictor in this model, in addition to culmen size.\nThis model ultimately performed very well new data, which is great. Since there was not 100% accuracy on the training data, I do wonder if there was an outlier in the training data."
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "This post covers my thoughts on the reading Solving the Equation: The variables for Women’s Success in Engineering and Computing, as well as a synopsis of the talks I attended and what I learned from attending the event\nI would like to note that the WiDS 2024 event was organized by me and Professor Tang from the math and statistics department as an ongiong effort to bring more data science events to Middlebury by highlighting the incredible data science work done by women. This is an opportunity for students to learn about the interdisciplenary applications of data sciecne skills and meet peers who are also interested in data science.\nThis event was such a success. The keynote from Professor Sarah Brown was incredible. A big takeaway I had was that a lot of the issues people run into when using machine learning doesn’t have to do with the algorithm or the code (maybe in Professor Biesters case wrangling all of Reddits posts it would be). Rather, the issues stem from misinterpretations of the data, data that lies about what it’s meant to represent, and challenges collecting best data to represent the issues at hand. The anecdote that stood out to me the most was Professor Browns story of collaborating with a social psychologist to ask people the appropriate questions to reveal bias. When working with such abstract topics such as bias and fairness, I was very impressed by how creative people were able to get to overcome unexpected obstacles.\n\n\n\n\n\n\n“Half our team, we’re not even putting on the field. We’ve got to change those numbers.” Former President Barack Obama, 2014 White House Science Fair\n\nUnderrepresentation of women in computing, math and engineering fileds creates stereotypes that women are worse than men in these fields, which leads to biases in decision making processes e.g. hiring. These stereotypes also lead to a negative feedback look where women become less engaged in the fields that they are negatively stereotyped in, or they perform worse at tasks when reminded of the negative stereotypes against them in those tasks. The presence of women at all levels of the organization can create more community for women, in addition to more female role models. Having more women in these fields would help reduce the implicit gender-science bias in both men and women. These issues do not lie solely apply to women. The biases that lead to underrepresenation impact everybody. As said by former president Barack Obama, less women in the field means there’s a lot of labor missing that could be there. More minds working on computing, mathm and engineering would benefit us all.\n\n\n\nRepresentation of women in computing grew from 1960 to 1990, but then actually declined back to 1960 levels by 2013 at just over 25% of workers. This is different from most other stem fields where, no matter how small the portion of women labor is, grew female participation from 1960 to 2013. Many large tech companies eg. Twitter, Google and Facebook had 20% or lower female participation. Starting in the mid-1900’s, both men and women start working clerical jobs and into the computer and IT sector. But then companies started using aptitude tests to get new hires, and these tests would favor men. Additionally, computer science programs began to get located in engineering colleges, which were histically all men (maybe 1% women). With the rise of personal computers, one could argue that men and boys clung onto a germa culture that became inaccessible for women. Another possibility to explain the drop of female participation in the computing sector is since so many people wanted to get computer science degrees, schools started restricting attendance, and these restrictions were often biased against women. This trend of low female representation in computing fields remains a problem to this day.\n\n\n\nThe section mentions many challenges that come from a lack of representation in STEM field. Isolation, which comes from a lack of voice and support in their field, can be avoided if there are regular events that highlight the presence of women in these fields in addition to their contributions and accomplishments. Another is stereotypes, and more specifically, microinequities. These effect the confidence of students overtime and can ultimately influence their career choice. Microinequities are often pointed towards women in STEM fields, and could deter them from pursuiting their studies in that field further, even if they are equally as qualified as their male peers. These stereotypes can also lead to biases that affect women in their jobs, whether it be the hiring process or performance reviews. Showcasing the work of women in the field is a powerful way of disassembling the stereotype that women don’t belong in the field and should not attempt to be. Evenmore, sexism and sexual harrasment are pervasive and can be masked by women who are trying to “fit in”. This ties into the final topic of this section where they describe many women feeling a lack of belonging in their field. If the work of women in STEM were better known, that would create a sense of belonging for women in the field and could reduce their willingness to let assault slide for the sake of fitting in.\n\n\n\n\nDone!\n\n\n\nWhat was talked about, what was learned eg. data sources, what was the result of the lecture, what are assumptions they made in their research, issues they came across\n\n\nHonestly, this topic was pretty foreign (pun not intended) to me. This is my best interpretation of the contents of the talk: RQ: Why do elected members continue to run for seats?/ what is their influence So few members of the audience truly understood the purpose of the UN Security Council and maybe that was due to the audience being a lot of non-political science majors, but Professor Yuen agreed to the sentiment- and if we know so little about this council, why are members rerunning for the council and what exactly is their influence? One of the facts she revealed is that the UN Security Council voting rules favor the powerful. Their data collection process involved interviewing with council members and looking at data to see why they want to rerun. First, they asked the question is the security countil democratic? They found that no, democracy is when everybody has an equal vote and only 5 people have a veto vote on the council, which means not everybodies vote is equal. They measured several things to answer who engages in the council, and whose issues get attention. Measured things include resolutions, formal meetings, who is sponsoring resolutions. Findings They have found that passed resolution sponsorship count was higher for permanent members, then sitting elected members (policy entrepreneurs), and very little from those not on council. By geographic region, council members from EAS and ECS (europeans and east africa) have a lot of influence. They found that the most frequently elected council members are Japan, then Brazil (these countries give a lot of money to the cause and are very active members). But these are up to change: Japan might not run again due to domestic issues, Canada used to be a frequent flyer before dropping off once they lost to Ireland. In summary, in terms of representation, they find that while the council is not democratic, it could be worse. Moving forward, they’re going to look into what this means for output and effectiveness, which could involve looking into what language gets put into resolutions etc.\nProfessor Yuen’s talk went into a lot of depth about this one research project of hers.\n\n\n\nI really liked Professor L’Roe’s talk because she talked about her experience as a woman coming from a background where gender stereotypes are enforced, and entering into stereotypically male, and male-biased field. The projects she talked about were all very insightful as well, because they highlighted an aspect of the data process that is sometimes overlooked, which is the data collection process before it’s in excel and csv form. She has collaborated with people all over the world, particularly South America and Africa, to collect deforestation data on the ground by going from plot of land to plot of land asking about land ownership.\nSomething interesting she noticed in one of the data sets she worked with is that people would lie about the size of their properties to be just under thresholds that, when crossed, would subject them to additional fees. I found this fascinating because there was an economics experiment done one a baby healthcare policy that found the policy was ineffective, but upon further exploration into the data, they found that the data was skewed in the sense that nurses were recording babies just under the weight threshold to give them additional help when they might have actually been over that threshold. This phenomenon clearly is not a one-time instance and it’s possible to appear to many different types of data sources.\nOther data sources Professor L’Roe uses are surveys and interviews with local. She claimed that her role is connecting people to pixels and telling their story, which I found to be a great way to describe the work with data that she does.\n\n\n\nProfessor Biester’s research explore the connection between language and mental health. Early research shows people with depressions more commonly use first-person pronouns. Now, it’s possible to take data from the internet to look into this. Within Natual Language Processing, this has become a topic of interest.\nIn her data, Professor Biester ran into some issues with data. One huge challenge of collecting text data is privacy, and so a lot of researchers use public data relying on “self reports” of mental illness to identify those illnesses. In 2013, De Choudhury used Machine Learning to predict depression before onset from Twitter data, followed by Yates et al. who applied Deep Learning to reddit posts for depression prediction. In Professor Biesters research, they collected their own data, using Reddit data (all of it). Using the assumption that users who self-report were once in the symptom onset / pre-diagnosis stage, they have data to train on to create models that can predict if other users are in the pre-diagnosis stage of depression. This research is ongoing. This talk lead me to wonder if these methods can be used for more than just depression. Could it be used to detect anxiety or PTSD?\n\n\n\nInlcude: topic, primary argument, what was learned\nProfessor Sarah Browns Big Question is making Machin Learning more fair. A lot of times when we learn to be data scientists, we learn the stats, the computer science, and then apply those to some sort of domain. But Professor Brown agues that domain expertise is an essential part of being a successful data scientist. While we can try to just work with people in the domain of interest, without the notion to do so and without and basic domain knowledge ourselves, we are unable to critically and creatively make a predictive model about that domain. So for Professor Brown, her next question became What can we put in a data scientists toolbox to help them make fair ML? or to allow them to collaborate better with domain experts? She dove into three lessons that helper her supass challenges in her data and machine learning process: ##### 1. Her first lesson came from her 2006 High School social studies class. They used context to understand primary sources, which resulted in the lesson: Understand data’s context In 2012, she was working on a project involving data for diagnosing patients with PTSD. If the patient scores above a threshold on a certain medical exam, they are considered to have PTSD. Otherwise, they don’t. The goal of this project was to provide a computerized exam to score patients. This is where the screen started glitching and I went to help fix the screen. I honestly don’t know how this one ended. But I know they ended up adding a classifier to their linear algorithm or something like that. Using the data’s contest to build a better model.\n\n\nShe learned in her Advanced Writing course in grad school that Disciplines are communities. Much later in her career, when giving talk to colleague about fariness in ML, she argued that forcing fariness in one way reveals bias in other ways. Another professor approached her with the research question What if this is used to understand how people are biased?. This lead to a deeper discussion with that professor about how her current data was being collected and she ended up adjusting her methodology after realizing the current literature that she was following was written by computer science researchers and the skills needed were not computer science, rather social psychology skills. She worked with more people in social psychology field to understand biases better and improve her data collection questions.\n\n\n\nHer final lesson was learnd while on the board for National Society of Black Engineers. The national chapter was trying to get chapters to implement projects but they never ended up getting implemented. They finally realized that Student organizations don’t have the level of assistance that the national level organizations had. So the had to Meet people where they are. Once she became a part of the machine learning field, she recognized an issue that when creating an ML model, people would often hesitate to make a model more fair once they learned how accurate it was. So Professor Brown set out to see if there is a way to indicate fariness before even fitting the model, so there is no trade off between accuracy and fairness, and the focus is entirely on the fairness of the model. Her solution to enforce fairness in models is to teach model auditing before teaching her students how to fit models\nI really enojoyed this talk because the lessons are absolutely ones that I will take away with me. I took an economic psychology class one J-Term and learning about how a simple question can result in biased answers and then hearing that same sentiment shared in the second lesson was such a full-circle moment. I absolutely agree that discipline expertise in the field of data science is way understated, and that to fully understand the machine learning process from the very start of data collection to the very end of making a model, understanding the social implications of the process is essential.\n\n\n\n\n\nI learned a lot from the reading. While I’ve always believed that striving for female representation in STEM fields is important, all the reasons listed in the reading were helpful to deepend my understanding of why exactly more women in STEM is so important.\nCollaborating with all of these phenomenal women to host this event has been such a wonderful experience, and honestly the majority of learning happened when I spoke to the speakers outside of the event. I thought the work that Lauren Vollmer-Forrow does is fascinating and it’s truly a shame that the alumni panel wasn’t longer. The statistical modelling that she does for the government is such a cool example of math skills that are learned at Middlebury being applied to inform policy decision.\nI hope to learn more about the methods of reducing bias in models. Hopefully that will be in the form of completing a blog post.\nSomething else for me to do is create a feedback form to hear from the audience- what speakers people enjoyed, what they wished there was more/less of, what is a snack you want to see at next years event?"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#abstract",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#abstract",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "This post covers my thoughts on the reading Solving the Equation: The variables for Women’s Success in Engineering and Computing, as well as a synopsis of the talks I attended and what I learned from attending the event\nI would like to note that the WiDS 2024 event was organized by me and Professor Tang from the math and statistics department as an ongiong effort to bring more data science events to Middlebury by highlighting the incredible data science work done by women. This is an opportunity for students to learn about the interdisciplenary applications of data sciecne skills and meet peers who are also interested in data science.\nThis event was such a success. The keynote from Professor Sarah Brown was incredible. A big takeaway I had was that a lot of the issues people run into when using machine learning doesn’t have to do with the algorithm or the code (maybe in Professor Biesters case wrangling all of Reddits posts it would be). Rather, the issues stem from misinterpretations of the data, data that lies about what it’s meant to represent, and challenges collecting best data to represent the issues at hand. The anecdote that stood out to me the most was Professor Browns story of collaborating with a social psychologist to ask people the appropriate questions to reveal bias. When working with such abstract topics such as bias and fairness, I was very impressed by how creative people were able to get to overcome unexpected obstacles."
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-1-why-spotlight-women-in-data-science",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "“Half our team, we’re not even putting on the field. We’ve got to change those numbers.” Former President Barack Obama, 2014 White House Science Fair\n\nUnderrepresentation of women in computing, math and engineering fileds creates stereotypes that women are worse than men in these fields, which leads to biases in decision making processes e.g. hiring. These stereotypes also lead to a negative feedback look where women become less engaged in the fields that they are negatively stereotyped in, or they perform worse at tasks when reminded of the negative stereotypes against them in those tasks. The presence of women at all levels of the organization can create more community for women, in addition to more female role models. Having more women in these fields would help reduce the implicit gender-science bias in both men and women. These issues do not lie solely apply to women. The biases that lead to underrepresenation impact everybody. As said by former president Barack Obama, less women in the field means there’s a lot of labor missing that could be there. More minds working on computing, mathm and engineering would benefit us all.\n\n\n\nRepresentation of women in computing grew from 1960 to 1990, but then actually declined back to 1960 levels by 2013 at just over 25% of workers. This is different from most other stem fields where, no matter how small the portion of women labor is, grew female participation from 1960 to 2013. Many large tech companies eg. Twitter, Google and Facebook had 20% or lower female participation. Starting in the mid-1900’s, both men and women start working clerical jobs and into the computer and IT sector. But then companies started using aptitude tests to get new hires, and these tests would favor men. Additionally, computer science programs began to get located in engineering colleges, which were histically all men (maybe 1% women). With the rise of personal computers, one could argue that men and boys clung onto a germa culture that became inaccessible for women. Another possibility to explain the drop of female participation in the computing sector is since so many people wanted to get computer science degrees, schools started restricting attendance, and these restrictions were often biased against women. This trend of low female representation in computing fields remains a problem to this day.\n\n\n\nThe section mentions many challenges that come from a lack of representation in STEM field. Isolation, which comes from a lack of voice and support in their field, can be avoided if there are regular events that highlight the presence of women in these fields in addition to their contributions and accomplishments. Another is stereotypes, and more specifically, microinequities. These effect the confidence of students overtime and can ultimately influence their career choice. Microinequities are often pointed towards women in STEM fields, and could deter them from pursuiting their studies in that field further, even if they are equally as qualified as their male peers. These stereotypes can also lead to biases that affect women in their jobs, whether it be the hiring process or performance reviews. Showcasing the work of women in the field is a powerful way of disassembling the stereotype that women don’t belong in the field and should not attempt to be. Evenmore, sexism and sexual harrasment are pervasive and can be masked by women who are trying to “fit in”. This ties into the final topic of this section where they describe many women feeling a lack of belonging in their field. If the work of women in STEM were better known, that would create a sense of belonging for women in the field and could reduce their willingness to let assault slide for the sake of fitting in."
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-2-attend-wids",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-2-attend-wids",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "Done!"
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-3-report",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-3-report",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "What was talked about, what was learned eg. data sources, what was the result of the lecture, what are assumptions they made in their research, issues they came across\n\n\nHonestly, this topic was pretty foreign (pun not intended) to me. This is my best interpretation of the contents of the talk: RQ: Why do elected members continue to run for seats?/ what is their influence So few members of the audience truly understood the purpose of the UN Security Council and maybe that was due to the audience being a lot of non-political science majors, but Professor Yuen agreed to the sentiment- and if we know so little about this council, why are members rerunning for the council and what exactly is their influence? One of the facts she revealed is that the UN Security Council voting rules favor the powerful. Their data collection process involved interviewing with council members and looking at data to see why they want to rerun. First, they asked the question is the security countil democratic? They found that no, democracy is when everybody has an equal vote and only 5 people have a veto vote on the council, which means not everybodies vote is equal. They measured several things to answer who engages in the council, and whose issues get attention. Measured things include resolutions, formal meetings, who is sponsoring resolutions. Findings They have found that passed resolution sponsorship count was higher for permanent members, then sitting elected members (policy entrepreneurs), and very little from those not on council. By geographic region, council members from EAS and ECS (europeans and east africa) have a lot of influence. They found that the most frequently elected council members are Japan, then Brazil (these countries give a lot of money to the cause and are very active members). But these are up to change: Japan might not run again due to domestic issues, Canada used to be a frequent flyer before dropping off once they lost to Ireland. In summary, in terms of representation, they find that while the council is not democratic, it could be worse. Moving forward, they’re going to look into what this means for output and effectiveness, which could involve looking into what language gets put into resolutions etc.\nProfessor Yuen’s talk went into a lot of depth about this one research project of hers.\n\n\n\nI really liked Professor L’Roe’s talk because she talked about her experience as a woman coming from a background where gender stereotypes are enforced, and entering into stereotypically male, and male-biased field. The projects she talked about were all very insightful as well, because they highlighted an aspect of the data process that is sometimes overlooked, which is the data collection process before it’s in excel and csv form. She has collaborated with people all over the world, particularly South America and Africa, to collect deforestation data on the ground by going from plot of land to plot of land asking about land ownership.\nSomething interesting she noticed in one of the data sets she worked with is that people would lie about the size of their properties to be just under thresholds that, when crossed, would subject them to additional fees. I found this fascinating because there was an economics experiment done one a baby healthcare policy that found the policy was ineffective, but upon further exploration into the data, they found that the data was skewed in the sense that nurses were recording babies just under the weight threshold to give them additional help when they might have actually been over that threshold. This phenomenon clearly is not a one-time instance and it’s possible to appear to many different types of data sources.\nOther data sources Professor L’Roe uses are surveys and interviews with local. She claimed that her role is connecting people to pixels and telling their story, which I found to be a great way to describe the work with data that she does.\n\n\n\nProfessor Biester’s research explore the connection between language and mental health. Early research shows people with depressions more commonly use first-person pronouns. Now, it’s possible to take data from the internet to look into this. Within Natual Language Processing, this has become a topic of interest.\nIn her data, Professor Biester ran into some issues with data. One huge challenge of collecting text data is privacy, and so a lot of researchers use public data relying on “self reports” of mental illness to identify those illnesses. In 2013, De Choudhury used Machine Learning to predict depression before onset from Twitter data, followed by Yates et al. who applied Deep Learning to reddit posts for depression prediction. In Professor Biesters research, they collected their own data, using Reddit data (all of it). Using the assumption that users who self-report were once in the symptom onset / pre-diagnosis stage, they have data to train on to create models that can predict if other users are in the pre-diagnosis stage of depression. This research is ongoing. This talk lead me to wonder if these methods can be used for more than just depression. Could it be used to detect anxiety or PTSD?\n\n\n\nInlcude: topic, primary argument, what was learned\nProfessor Sarah Browns Big Question is making Machin Learning more fair. A lot of times when we learn to be data scientists, we learn the stats, the computer science, and then apply those to some sort of domain. But Professor Brown agues that domain expertise is an essential part of being a successful data scientist. While we can try to just work with people in the domain of interest, without the notion to do so and without and basic domain knowledge ourselves, we are unable to critically and creatively make a predictive model about that domain. So for Professor Brown, her next question became What can we put in a data scientists toolbox to help them make fair ML? or to allow them to collaborate better with domain experts? She dove into three lessons that helper her supass challenges in her data and machine learning process: ##### 1. Her first lesson came from her 2006 High School social studies class. They used context to understand primary sources, which resulted in the lesson: Understand data’s context In 2012, she was working on a project involving data for diagnosing patients with PTSD. If the patient scores above a threshold on a certain medical exam, they are considered to have PTSD. Otherwise, they don’t. The goal of this project was to provide a computerized exam to score patients. This is where the screen started glitching and I went to help fix the screen. I honestly don’t know how this one ended. But I know they ended up adding a classifier to their linear algorithm or something like that. Using the data’s contest to build a better model.\n\n\nShe learned in her Advanced Writing course in grad school that Disciplines are communities. Much later in her career, when giving talk to colleague about fariness in ML, she argued that forcing fariness in one way reveals bias in other ways. Another professor approached her with the research question What if this is used to understand how people are biased?. This lead to a deeper discussion with that professor about how her current data was being collected and she ended up adjusting her methodology after realizing the current literature that she was following was written by computer science researchers and the skills needed were not computer science, rather social psychology skills. She worked with more people in social psychology field to understand biases better and improve her data collection questions.\n\n\n\nHer final lesson was learnd while on the board for National Society of Black Engineers. The national chapter was trying to get chapters to implement projects but they never ended up getting implemented. They finally realized that Student organizations don’t have the level of assistance that the national level organizations had. So the had to Meet people where they are. Once she became a part of the machine learning field, she recognized an issue that when creating an ML model, people would often hesitate to make a model more fair once they learned how accurate it was. So Professor Brown set out to see if there is a way to indicate fariness before even fitting the model, so there is no trade off between accuracy and fairness, and the focus is entirely on the fairness of the model. Her solution to enforce fairness in models is to teach model auditing before teaching her students how to fit models\nI really enojoyed this talk because the lessons are absolutely ones that I will take away with me. I took an economic psychology class one J-Term and learning about how a simple question can result in biased answers and then hearing that same sentiment shared in the second lesson was such a full-circle moment. I absolutely agree that discipline expertise in the field of data science is way understated, and that to fully understand the machine learning process from the very start of data collection to the very end of making a model, understanding the social implications of the process is essential."
  },
  {
    "objectID": "posts/WiDS Blog Post/Blog2WiDS.html#part-4-reflection",
    "href": "posts/WiDS Blog Post/Blog2WiDS.html#part-4-reflection",
    "title": "WiDS Conference 2024",
    "section": "",
    "text": "I learned a lot from the reading. While I’ve always believed that striving for female representation in STEM fields is important, all the reasons listed in the reading were helpful to deepend my understanding of why exactly more women in STEM is so important.\nCollaborating with all of these phenomenal women to host this event has been such a wonderful experience, and honestly the majority of learning happened when I spoke to the speakers outside of the event. I thought the work that Lauren Vollmer-Forrow does is fascinating and it’s truly a shame that the alumni panel wasn’t longer. The statistical modelling that she does for the government is such a cool example of math skills that are learned at Middlebury being applied to inform policy decision.\nI hope to learn more about the methods of reducing bias in models. Hopefully that will be in the form of completing a blog post.\nSomething else for me to do is create a feedback form to hear from the audience- what speakers people enjoyed, what they wished there was more/less of, what is a snack you want to see at next years event?"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/WiDS Conference Blog?/WiDSConference_notes.html",
    "href": "posts/WiDS Conference Blog?/WiDSConference_notes.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#WiDS 24 Stanford conference\n** What I learned and who I met\nKelly Stroh- works at a pet insurance company discussed potential bias in the ML algorithems used to determine insurance policy when it comes to pet nicknames from other languages that the algorithm might not be able to discern.\nS- WiDS ambassador from Tel Aviv but works at Intuit, specifically working with the algorithms that read in information from tax forms (Turbo tax)\nMariana- WiDS ambassador from Lima Peru, doing her undergrad thesis on the Peruvian sign language which involves collecting video data of all of the alphabet signs and then programming them\nEllen Pao- former Kleiner Perkings employee who sued them for discrimmination, became interim Reddit CEO and now has started Project Interlude\nEmily Gordon- Stanford Post-doc, we can create models of the worlds climate systems to isolate the effect of CO2 and other agents on global warming\nAlice chen zhang Ford data science manager working on charging/energy services and cycle plan How can data scientist contribute to this shift to all-electric vehicles? Using big data and AI/ML to support business decision making and develop services. 1) enhancing the public charging experience. Problems are 1 in 5 EV charges fail, charging rn is slow. Challenges in solving this were in data collection process. There is a lack of data and noise in the data. Instead, planned for new data collection and decode, transform and build algorithms to clean data Challenges in modeling: cluter Goe locations using DBSCAN method, but werent sure how to set up the parameters and how frequently the clustering analysis should have been increased Also, when modelling site reliability, how to assign a reliability score with little to no pre-existing data? how much shuold the penalty be for a failed charge? How to measure score confidence? Still working with engineers to classify failures to properly address the issues. Developing an algorithm to detect failure patterns to identify the issues before they even happen and using genAI to identify customer pain points\n\nDigital Economies\nNicole Carignan Strategic Cyber AI, Darktrace Combating cyber attacks involves using ML to recognize attack patterns and usign LLm to find patterns of language use to classify the buisness patterns of life (unsupervised ML) to use against novel, nation , and a whole variety of attacks\nSpecifically, uses unsupervised ML, bayesian, Deep neural networks, clustering, graph theory, anomaly detection\nWant to detect Purple Fox using darktrace AI\nZanele Munyikwa Navigating future of work in the digital era Spoke about task-based framework Tech impact on labor: - Displacement effect - decrease in H D for labor as AI replaces - Reinstatement Effect - machines need new tasks to operate the machines - Productivity Effect - substituation of labor on\nRole of Data: - Administrative using O*NET and Bureau and Labor stats - Data from experiments - Jpb postings, employee reviews, employment reports\nCase Study: GenAI how does GenAI effect jobs in copywriting 3 treatments: no Ai, some AI, full AI help Assessed outcomes using: time spent writing ads, vocab diversity, etc. Outcome: more augmentations = less time spent and less subjective ownership What this means: concerns around job quality, job satisfaction. Ineq in the digital era\nSandra Moerch Is AI killing our purpose Behind every design is an idea, and every idea has a purpose AI can help ideate, but cannot help us find purpose\nprompted genAI to draw engineer using genAI, and that came up with a white male every time. While unfortunate, this means we need to get more creative with our prompts. Generative prompter is a growing field.\nGenerative design Ai can be used as a creative assistant- good for some, but bad for jobs\nJoining forces w AI vs trying to beat it\nIn the break, met another woman working at Intuit who helps with making sure the data science teams are seamlessly running fairness checks on the models. That was great to hear that companies are implementing these checks and finding them important.\nFord- hearing this talk lead me to\nKimberly Babiarz using data science and remote sensing to detect where human trafficking is occuring in remote areas of Brazil\nRebecca Portnoff is the CEO of Thorn, which is a non-profit that created a model to detect child sexual abuse on social media platforms. One challenge they came across that I found fascinating was that they ran into an issue training a model on CSA material when storing that data is illegal. They solved this by working directly with platforms to host the data and train their model on.\nSpoke with Zanele about her research at MIT and she offered some insight into the fact that AI is impacting men vs women differently, in that men and women have historically performed different tasks and therefore now when AI and robots replace people in certain tasks, it has an unequal effect on men and women. Could be an interesting research topic to look into for ECON final project.\nWiDS datathon for blog post? creating ML to predict something using oncology data from Gilead"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWiDS Conference 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSCI 0451: Mid-Course Reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigital Economies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Breanna Guo",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Logistic Regression Blog Post/LogisticRegression_Blog.html",
    "href": "posts/Logistic Regression Blog Post/LogisticRegression_Blog.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: https://github.com/breannaguo/csci451/blob/main/posts/Logistic%20Regression%20Blog%20Post/logistic.py\nIn this Blog Post, my aim is to - implement gradient descent for logistic regression in an object oriented paradigm - implemenet a key variant of gradient descent with momentum in order to achieve faster convergence - perform experiments to test these implementations\nIn logistic.py, I implement gradient descent for logistic regression. I then explore this in the following “Experimenting with data section”, where I run regressions with and without momentum to observe the effects of momentum on gradeitn descent. I find that including momentum leads to much faster convergence. Finally, I explore how logistic regression may be subject to overfitting on training data. I use an example where, when tested on training data, the model achieves almost 100% accuracy, while the testing data performs significantly worse.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nfrom matplotlib import pyplot as plt\nimport torch\n\n\n\nFirst, generating some data to classify\nThen, experimenting - with “normal” gradient descent where \\(\\beta = 0\\) - with momentum, \\(\\beta &gt; 0\\)\n\n# The following code to create data points was prepared by Prof. Chodrow for this assignment\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nPlotting the data\n\ndef plotData(X, y, ax):\n    targets = [0, 1]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"spring\", vmin = -2, vmax = 2, alpha = 0.7)\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \nX, y = classification_data(noise = 0.2)\nfig, ax = plt.subplots(1,1)\n\nplotData(X,y,ax)\n\n\n\n\n\n\n\n\nThere is clear linear separability in this data!\n\n\n\nWhen the number of features \\(p_{dim} = 2\\), when \\(\\alpha\\) is sufficiently small and \\(\\beta = 0\\), gradient descent for logistic regression converges to a weight vector w that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations). This is a good experiment to use to assess whether the linear regression or the gradient descent has bugs.\nThe loss function is:\n\\(w_{k+1} \\leftarrow w_k - \\alpha\\bigtriangledown L(w_k)\\)\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n#initalize vector to keep track of loss over iterations\nloss_vec = []\n\niterations = 300\nfor iter in range(iterations):\n    opt.step(X, y, alpha = 0.5, beta = 0)\n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n\n\ndef plot_loss_fnc(loss):\n    plt.plot(loss)\n    plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\n    plt.title(\"Logistic Regression Loss over \" + str(iterations) + \" Iterations\")\n\n\nplot_loss_fnc(loss = loss_vec)\n\n\n\n\n\n\n\n\nInterestingly, sometimes loss is monotonic and sometimes it isn’t…\nNow, adding the decision boundary to the data points\n\n# function to draw decision boundary\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\nfig, ax = plt.subplots(1,1)\nplotData(X, y, ax)\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"darkgrey\")\n\n\n\n\n\n\n\n\nThe decision boundary found using normal gradient descent perfectly splits the data.\n\n\n\nOn the same data, gradient descent with momentum (e.g. \\(\\beta = 0.9\\)) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with \\(\\beta = 0\\)). Plot the loss over iterations for each method. You may need to experiment with the data and choice of \\(\\alpha\\) in order to observe speedups due to momentum.\nLoss function with momentum:\n\\(w_{k+1} \\leftarrow w_k - \\alpha\\bigtriangledown L(w_k) + \\beta (w_k - w_{k-1})\\)\nNotice that the loss function without momentum is whe \\(\\beta = 0\\) and the last component of the function is cancelled\nTraining a logistic regression model with with \\(\\beta = 0.9\\)\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n#initalize vector to keep track of loss over iterations\nloss_vec_momentum = []\n\niterations = 300\nfor iter in range(iterations):\n    opt.step(X, y, alpha = 0.9, beta = 0.9)\n    loss = LR.loss(X, y).item()\n    loss_vec_momentum.append(loss)\n\n\nplot_loss_fnc(loss = loss_vec_momentum)\nplot_loss_fnc(loss = loss_vec)\nplt.legend([\"Loss with Momentum\", \"Loss without Momentum\"], loc = \"upper right\" )\n\n\n\n\n\n\n\n\nIt’s clear from this graph that the rate at which loss is decreasing over iterations is much steeper compared to when momentum was not used!\n\n\n\nGenerate some data where p_dim &gt; n_points. For example, p_dim = 100 and n_points = 50. Do this twice with the exact same parameters. Call the first dataset X_train, y_train and the second dataset X_test, y_test. Then, do an experiment in which you fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data. What is the accuracy on the test data?\nOverfitting is an issue that occur with any type of machine learning algorithm, and logistic regression is particularly prone to overfitting.\n\n#generating train and test data where p_dim &gt; n_points\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\niterations = 100\nfor iter in range(iterations):\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\n\n# Training data accuracy\n(1.0*(LR.predict(X_train) == y_train)).mean()\n\ntensor(1.)\n\n\nThe training data is able to get 100% accuracy\n\n# Test data accuracy\n(1.0*(LR.predict(X_test) == y_test)).mean()\n\ntensor(0.9400)\n\n\nThe test data does significantly worse (6 percentage points) than the training data, showing how the model has been overfit\n\n\n\nThis exploration of logistic regression has allowed me to observe the impact of momentum on the models ability to converge quickly, in addition to the models tendency to overfit if not careful. While I noticed that loss was not monotonic, it did decrease significantly over 300 iterations, and the rate at which loss decreased was impacted by the momentum \\(\\beta\\). Ultimately, this was a very successful exploration."
  },
  {
    "objectID": "posts/Logistic Regression Blog Post/LogisticRegression_Blog.html#april-28th-2024",
    "href": "posts/Logistic Regression Blog Post/LogisticRegression_Blog.html#april-28th-2024",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Here is the link to my source code: https://github.com/breannaguo/csci451/blob/main/posts/Logistic%20Regression%20Blog%20Post/logistic.py\nIn this Blog Post, my aim is to - implement gradient descent for logistic regression in an object oriented paradigm - implemenet a key variant of gradient descent with momentum in order to achieve faster convergence - perform experiments to test these implementations\nIn logistic.py, I implement gradient descent for logistic regression. I then explore this in the following “Experimenting with data section”, where I run regressions with and without momentum to observe the effects of momentum on gradeitn descent. I find that including momentum leads to much faster convergence. Finally, I explore how logistic regression may be subject to overfitting on training data. I use an example where, when tested on training data, the model achieves almost 100% accuracy, while the testing data performs significantly worse.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nfrom matplotlib import pyplot as plt\nimport torch\n\n\n\nFirst, generating some data to classify\nThen, experimenting - with “normal” gradient descent where \\(\\beta = 0\\) - with momentum, \\(\\beta &gt; 0\\)\n\n# The following code to create data points was prepared by Prof. Chodrow for this assignment\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nPlotting the data\n\ndef plotData(X, y, ax):\n    targets = [0, 1]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"spring\", vmin = -2, vmax = 2, alpha = 0.7)\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \nX, y = classification_data(noise = 0.2)\nfig, ax = plt.subplots(1,1)\n\nplotData(X,y,ax)\n\n\n\n\n\n\n\n\nThere is clear linear separability in this data!\n\n\n\nWhen the number of features \\(p_{dim} = 2\\), when \\(\\alpha\\) is sufficiently small and \\(\\beta = 0\\), gradient descent for logistic regression converges to a weight vector w that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations). This is a good experiment to use to assess whether the linear regression or the gradient descent has bugs.\nThe loss function is:\n\\(w_{k+1} \\leftarrow w_k - \\alpha\\bigtriangledown L(w_k)\\)\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n#initalize vector to keep track of loss over iterations\nloss_vec = []\n\niterations = 300\nfor iter in range(iterations):\n    opt.step(X, y, alpha = 0.5, beta = 0)\n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n\n\ndef plot_loss_fnc(loss):\n    plt.plot(loss)\n    plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\n    plt.title(\"Logistic Regression Loss over \" + str(iterations) + \" Iterations\")\n\n\nplot_loss_fnc(loss = loss_vec)\n\n\n\n\n\n\n\n\nInterestingly, sometimes loss is monotonic and sometimes it isn’t…\nNow, adding the decision boundary to the data points\n\n# function to draw decision boundary\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\nfig, ax = plt.subplots(1,1)\nplotData(X, y, ax)\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"darkgrey\")\n\n\n\n\n\n\n\n\nThe decision boundary found using normal gradient descent perfectly splits the data.\n\n\n\nOn the same data, gradient descent with momentum (e.g. \\(\\beta = 0.9\\)) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with \\(\\beta = 0\\)). Plot the loss over iterations for each method. You may need to experiment with the data and choice of \\(\\alpha\\) in order to observe speedups due to momentum.\nLoss function with momentum:\n\\(w_{k+1} \\leftarrow w_k - \\alpha\\bigtriangledown L(w_k) + \\beta (w_k - w_{k-1})\\)\nNotice that the loss function without momentum is whe \\(\\beta = 0\\) and the last component of the function is cancelled\nTraining a logistic regression model with with \\(\\beta = 0.9\\)\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n#initalize vector to keep track of loss over iterations\nloss_vec_momentum = []\n\niterations = 300\nfor iter in range(iterations):\n    opt.step(X, y, alpha = 0.9, beta = 0.9)\n    loss = LR.loss(X, y).item()\n    loss_vec_momentum.append(loss)\n\n\nplot_loss_fnc(loss = loss_vec_momentum)\nplot_loss_fnc(loss = loss_vec)\nplt.legend([\"Loss with Momentum\", \"Loss without Momentum\"], loc = \"upper right\" )\n\n\n\n\n\n\n\n\nIt’s clear from this graph that the rate at which loss is decreasing over iterations is much steeper compared to when momentum was not used!\n\n\n\nGenerate some data where p_dim &gt; n_points. For example, p_dim = 100 and n_points = 50. Do this twice with the exact same parameters. Call the first dataset X_train, y_train and the second dataset X_test, y_test. Then, do an experiment in which you fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data. What is the accuracy on the test data?\nOverfitting is an issue that occur with any type of machine learning algorithm, and logistic regression is particularly prone to overfitting.\n\n#generating train and test data where p_dim &gt; n_points\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\niterations = 100\nfor iter in range(iterations):\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\n\n# Training data accuracy\n(1.0*(LR.predict(X_train) == y_train)).mean()\n\ntensor(1.)\n\n\nThe training data is able to get 100% accuracy\n\n# Test data accuracy\n(1.0*(LR.predict(X_test) == y_test)).mean()\n\ntensor(0.9400)\n\n\nThe test data does significantly worse (6 percentage points) than the training data, showing how the model has been overfit\n\n\n\nThis exploration of logistic regression has allowed me to observe the impact of momentum on the models ability to converge quickly, in addition to the models tendency to overfit if not careful. While I noticed that loss was not monotonic, it did decrease significantly over 300 iterations, and the rate at which loss decreased was impacted by the momentum \\(\\beta\\). Ultimately, this was a very successful exploration."
  }
]